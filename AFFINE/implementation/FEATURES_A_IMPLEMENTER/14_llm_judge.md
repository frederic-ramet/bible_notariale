# ‚öñÔ∏è Am√©lioration #13 : LLM-as-a-Judge

[‚Üê Retour √† l'index](./00_INDEX.md)

---

## üìä Fiche technique

| Attribut | Valeur |
|----------|--------|
| **Priorit√©** | üü¢ MOYEN |
| **Impact** | ‚≠ê‚≠ê‚≠ê (Monitoring qualit√©) |
| **Effort** | 1 jour |
| **Statut** | üìã √Ä faire |
| **D√©pendances** | Toutes les autres am√©liorations |
| **Repo** | `application` |

---

## üéØ Probl√®me identifi√©

### Observations

**Probl√®me** : Pas de mesure automatique de la qualit√© des r√©ponses

**Sympt√¥mes** :
- √âvaluation manuelle co√ªteuse (Julien teste 15 questions manuellement)
- Pas de monitoring continu de la qualit√©
- R√©gressions d√©tect√©es tardivement
- Impossible de mesurer impact des am√©liorations √† grande √©chelle

**Impact** :
- ‚ùå D√©tection de r√©gressions tardive
- ‚ùå Pas de m√©triques automatiques
- ‚ùå Validation manuelle chronophage
- ‚ùå Impossible d'A/B tester √† grande √©chelle

**Exemple concret** :

```
Question : "Combien de cong√©s pay√©s ai-je en tant que clerc ?"

R√©ponse g√©n√©r√©e : "Selon la CCN Notariat, les clercs b√©n√©ficient de 30 jours
ouvrables de cong√©s pay√©s par an, acquis √† raison de 2.5 jours par mois."

‚ùå Sans LLM-as-a-Judge :
- Impossible de savoir si cette r√©ponse est bonne automatiquement
- Besoin d'un humain pour valider

‚úÖ Avec LLM-as-a-Judge :
- LLM √©value automatiquement la r√©ponse selon crit√®res :
  * Exactitude : 9/10 (info correcte)
  * Compl√©tude : 8/10 (manque info sur p√©riode de r√©f√©rence)
  * Format : 10/10 (structure claire)
  * Sources : 10/10 (CCN cit√©e)
- Score global : 9.25/10
- Feedback : "R√©ponse correcte mais pourrait mentionner la p√©riode de r√©f√©rence"
```

---

## üí° Solution propos√©e

### Vue d'ensemble

**LLM-as-a-Judge : √âvaluation automatique des r√©ponses** :

1. **Apr√®s chaque r√©ponse** : LLM juge √©value la qualit√©
2. **Crit√®res multiples** : Exactitude, compl√©tude, format, sources
3. **Score et feedback** : Note 0-10 + explication
4. **Monitoring continu** : Dashboard avec m√©triques temps r√©el

### Architecture

```mermaid
graph LR
    A[Question] --> B[RAG g√©n√®re r√©ponse]
    B --> C[R√©ponse]
    C --> D[LLM Judge √©value]
    D --> E[Score + Feedback]
    E --> F[Logs monitoring]
    E --> G{Score < 6 ?}
    G -->|Oui| H[Alerte]
    G -->|Non| I[OK]
```

---

## üîß Impl√©mentation d√©taill√©e

### Nouveau service : `services/llm_judge.py`

```python
"""
LLM-as-a-Judge : √âvaluation automatique de la qualit√© des r√©ponses
"""

from typing import Dict, List
from dataclasses import dataclass
import json


@dataclass
class JudgeScore:
    """Score d'√©valuation du juge"""

    # Scores par crit√®re (0-10)
    exactitude: float  # Information factuelle correcte ?
    completude: float  # R√©pond compl√®tement √† la question ?
    format: float  # Format APRES respect√© ?
    sources: float  # Sources cit√©es et pertinentes ?

    # Score global
    score_global: float

    # Feedback textuel
    feedback: str

    # D√©tails
    strengths: List[str]  # Points forts
    weaknesses: List[str]  # Points faibles
    suggestions: List[str]  # Am√©liorations sugg√©r√©es


class LLMJudge:
    """
    √âvalue la qualit√© des r√©ponses g√©n√©r√©es
    """

    def __init__(self, openai_client):
        self.client = openai_client

    async def evaluate(
        self,
        question: str,
        answer: str,
        context_chunks: List[dict]
    ) -> JudgeScore:
        """
        √âvalue une r√©ponse

        Args:
            question: Question pos√©e
            answer: R√©ponse g√©n√©r√©e
            context_chunks: Chunks utilis√©s pour g√©n√©rer la r√©ponse

        Returns:
            Score d'√©valuation
        """

        # Construire le prompt d'√©valuation
        prompt = self._build_judge_prompt(question, answer, context_chunks)

        # Appeler LLM juge
        response = await self.client.chat.completions.create(
            model="gpt-4o",  # Mod√®le fort pour juger
            messages=[
                {"role": "system", "content": JUDGE_SYSTEM_PROMPT},
                {"role": "user", "content": prompt}
            ],
            temperature=0.1,  # Faible temp√©rature pour coh√©rence
            max_tokens=500
        )

        # Parser la r√©ponse JSON
        judge_response = response.choices[0].message.content.strip()

        try:
            scores_dict = json.loads(judge_response)
        except json.JSONDecodeError:
            # Fallback si JSON mal form√©
            scores_dict = {
                'exactitude': 5.0,
                'completude': 5.0,
                'format': 5.0,
                'sources': 5.0,
                'feedback': 'Erreur parsing r√©ponse juge',
                'strengths': [],
                'weaknesses': ['Erreur √©valuation'],
                'suggestions': []
            }

        # Calculer score global
        score_global = (
            scores_dict['exactitude'] * 0.4 +
            scores_dict['completude'] * 0.3 +
            scores_dict['format'] * 0.15 +
            scores_dict['sources'] * 0.15
        )

        return JudgeScore(
            exactitude=scores_dict['exactitude'],
            completude=scores_dict['completude'],
            format=scores_dict['format'],
            sources=scores_dict['sources'],
            score_global=score_global,
            feedback=scores_dict.get('feedback', ''),
            strengths=scores_dict.get('strengths', []),
            weaknesses=scores_dict.get('weaknesses', []),
            suggestions=scores_dict.get('suggestions', [])
        )

    def _build_judge_prompt(
        self,
        question: str,
        answer: str,
        context_chunks: List[dict]
    ) -> str:
        """
        Construit le prompt d'√©valuation
        """

        # Formatter les chunks de contexte
        context_summary = "\n".join([
            f"- {chunk.get('doc_titre', 'Document')}: {chunk.get('text', '')[:200]}..."
            for chunk in context_chunks[:3]  # Limiter √† 3 chunks
        ])

        prompt = f"""√âvalue la qualit√© de cette r√©ponse selon les crit√®res d√©finis.

**Question pos√©e** :
{question}

**R√©ponse g√©n√©r√©e** :
{answer}

**Contexte documentaire utilis√©** :
{context_summary}

√âvalue la r√©ponse selon 4 crit√®res (note de 0 √† 10 pour chaque) :

1. **Exactitude** : Les informations sont-elles factuellement correctes ?
2. **Compl√©tude** : La r√©ponse r√©pond-elle compl√®tement √† la question ?
3. **Format** : Le format APRES (Analyse, Principe, R√®gle, Sources) est-il respect√© ?
4. **Sources** : Les sources sont-elles cit√©es, pertinentes et v√©rifiables ?

R√©ponds UNIQUEMENT avec un JSON dans ce format :

{{
  "exactitude": <score 0-10>,
  "completude": <score 0-10>,
  "format": <score 0-10>,
  "sources": <score 0-10>,
  "feedback": "Synth√®se en 1-2 phrases",
  "strengths": ["point fort 1", "point fort 2"],
  "weaknesses": ["point faible 1"],
  "suggestions": ["suggestion 1"]
}}
"""

        return prompt


# System prompt pour le juge
JUDGE_SYSTEM_PROMPT = """Tu es un expert en √©valuation de r√©ponses juridiques et notariales.

Ta mission : √âvaluer la qualit√© des r√©ponses g√©n√©r√©es par un chatbot notarial.

Crit√®res d'√©valuation :

1. **Exactitude** (0-10) :
   - 10 : Information 100% correcte, v√©rifiable dans le contexte
   - 7-9 : Information correcte avec nuances mineures manquantes
   - 4-6 : Information partiellement correcte
   - 0-3 : Information incorrecte ou hallucination

2. **Compl√©tude** (0-10) :
   - 10 : R√©pond compl√®tement √† tous les aspects de la question
   - 7-9 : R√©pond aux aspects principaux, manque d√©tails secondaires
   - 4-6 : R√©pond partiellement
   - 0-3 : Ne r√©pond pas ou r√©ponse hors sujet

3. **Format** (0-10) :
   - 10 : Format APRES parfaitement respect√© (Analyse, Principe, R√®gle, Sources)
   - 7-9 : Format APRES pr√©sent avec sections identifiables
   - 4-6 : Format partiel
   - 0-3 : Pas de structure

4. **Sources** (0-10) :
   - 10 : Sources cit√©es, pr√©cises (article, num√©ro), v√©rifiables dans le contexte
   - 7-9 : Sources cit√©es mais r√©f√©rences impr√©cises
   - 4-6 : Mention de sources sans pr√©cision
   - 0-3 : Pas de sources cit√©es

Sois objectif et rigoureux dans ton √©valuation.
"""
```

---

### Int√©gration dans RAG : `services/notaria_rag_service.py`

```python
"""
Int√©gration LLM Judge dans le RAG
"""

from services.llm_judge import LLMJudge

class NotariaRAGService:

    def __init__(self):
        # ... autres initialisations
        self.judge = LLMJudge(self.openai_client)

    async def generate_answer(
        self,
        question: str,
        chunks: List[dict],
        intent: str,
        evaluate: bool = True  # Flag pour activer/d√©sactiver √©valuation
    ) -> dict:
        """
        G√©n√®re une r√©ponse avec √©valuation qualit√©
        """

        # 1. G√©n√©rer r√©ponse (code existant)
        answer = await self._generate_answer_internal(question, chunks, intent)

        response = {
            "answer": answer,
            "cited_sources": [],  # ...
            "chunks_used": len(chunks)
        }

        # 2. √âvaluer la r√©ponse si demand√©
        if evaluate:
            judge_score = await self.judge.evaluate(question, answer, chunks)

            response['quality_score'] = {
                'exactitude': judge_score.exactitude,
                'completude': judge_score.completude,
                'format': judge_score.format,
                'sources': judge_score.sources,
                'score_global': judge_score.score_global,
                'feedback': judge_score.feedback
            }

            # Logger le score
            await self._log_quality_score(question, answer, judge_score)

            # Alerte si score faible
            if judge_score.score_global < 6.0:
                await self._alert_low_quality(question, answer, judge_score)

        return response

    async def _log_quality_score(
        self,
        question: str,
        answer: str,
        judge_score: JudgeScore
    ):
        """
        Log le score de qualit√© dans la base
        """

        await db.insert('quality_scores', {
            'timestamp': datetime.now(),
            'question': question,
            'answer': answer[:500],  # Tronquer si long
            'exactitude': judge_score.exactitude,
            'completude': judge_score.completude,
            'format': judge_score.format,
            'sources': judge_score.sources,
            'score_global': judge_score.score_global,
            'feedback': judge_score.feedback
        })

    async def _alert_low_quality(
        self,
        question: str,
        answer: str,
        judge_score: JudgeScore
    ):
        """
        Alerte si r√©ponse de faible qualit√©
        """

        logger.warning(f"""
‚ö†Ô∏è  R√âPONSE DE FAIBLE QUALIT√â D√âTECT√âE
Score global : {judge_score.score_global}/10

Question : {question}
Feedback : {judge_score.feedback}

Points faibles :
{chr(10).join(f'- {w}' for w in judge_score.weaknesses)}

Suggestions :
{chr(10).join(f'- {s}' for s in judge_score.suggestions)}
        """)

        # Optionnel : envoyer notification Slack/email
```

---

## üìä Dashboard de monitoring

### Script : `scripts/generate_quality_dashboard.py`

```python
"""
G√©n√®re un dashboard de qualit√© depuis les logs
"""

import asyncio
from datetime import datetime, timedelta


async def generate_quality_report(days: int = 7):
    """
    G√©n√®re un rapport de qualit√© sur les N derniers jours
    """

    # R√©cup√©rer scores depuis la base
    scores = await db.query("""
        SELECT *
        FROM quality_scores
        WHERE timestamp >= NOW() - INTERVAL '{days} days'
        ORDER BY timestamp DESC
    """.format(days=days))

    # Statistiques globales
    total = len(scores)
    avg_exactitude = sum(s['exactitude'] for s in scores) / total
    avg_completude = sum(s['completude'] for s in scores) / total
    avg_format = sum(s['format'] for s in scores) / total
    avg_sources = sum(s['sources'] for s in scores) / total
    avg_global = sum(s['score_global'] for s in scores) / total

    # R√©partition par tranche de score
    distribution = {
        'Excellent (>8)': sum(1 for s in scores if s['score_global'] > 8),
        'Bon (6-8)': sum(1 for s in scores if 6 <= s['score_global'] <= 8),
        'Faible (<6)': sum(1 for s in scores if s['score_global'] < 6)
    }

    # Top 5 pires r√©ponses
    worst_responses = sorted(scores, key=lambda s: s['score_global'])[:5]

    # G√©n√©rer rapport markdown
    report = f"""# üìä Rapport Qualit√© - {days} derniers jours

**P√©riode** : {datetime.now() - timedelta(days=days)} ‚Üí {datetime.now()}
**Total r√©ponses √©valu√©es** : {total}

---

## üéØ Scores moyens

| Crit√®re | Score moyen |
|---------|-------------|
| **Exactitude** | {avg_exactitude:.2f}/10 |
| **Compl√©tude** | {avg_completude:.2f}/10 |
| **Format** | {avg_format:.2f}/10 |
| **Sources** | {avg_sources:.2f}/10 |
| **Score global** | {avg_global:.2f}/10 |

---

## üìà R√©partition

| Tranche | Nombre | Pourcentage |
|---------|--------|-------------|
| Excellent (>8) | {distribution['Excellent (>8)']} | {distribution['Excellent (>8)']/total*100:.1f}% |
| Bon (6-8) | {distribution['Bon (6-8)']} | {distribution['Bon (6-8)']/total*100:.1f}% |
| Faible (<6) | {distribution['Faible (<6)']} | {distribution['Faible (<6)']/total*100:.1f}% |

---

## ‚ö†Ô∏è  Top 5 pires r√©ponses

"""

    for i, resp in enumerate(worst_responses, 1):
        report += f"""
### {i}. Score : {resp['score_global']:.2f}/10

**Question** : {resp['question']}
**Feedback** : {resp['feedback']}
**Timestamp** : {resp['timestamp']}

---
"""

    # Sauvegarder rapport
    with open(f'quality_report_{datetime.now().strftime("%Y%m%d")}.md', 'w') as f:
        f.write(report)

    print(f"‚úÖ Rapport g√©n√©r√© : quality_report_{datetime.now().strftime('%Y%m%d')}.md")


if __name__ == '__main__':
    asyncio.run(generate_quality_report(days=7))
```

---

## ‚úÖ Tests et validation

### Tests unitaires

```python
"""
Tests pour LLM Judge
"""

import pytest
from services.llm_judge import LLMJudge

@pytest.mark.asyncio
async def test_evaluate_good_answer(openai_client):
    """Test √©valuation d'une bonne r√©ponse"""

    judge = LLMJudge(openai_client)

    question = "Combien de cong√©s pay√©s ai-je ?"

    answer = """**Analyse** : La question porte sur le nombre de jours de cong√©s pay√©s.

**Principe** : Selon la CCN Notariat, les clercs b√©n√©ficient de cong√©s pay√©s annuels.

**R√®gle** :
- 30 jours ouvrables de cong√©s pay√©s par an
- Acquis √† raison de 2.5 jours par mois de travail effectif

**Sources** :
- CCN Notariat - Article 45 (Cong√©s pay√©s)
"""

    chunks = [
        {'doc_titre': 'CCN Article 45', 'text': 'Les clercs b√©n√©ficient de 30 jours...'}
    ]

    score = await judge.evaluate(question, answer, chunks)

    # V√©rifier scores √©lev√©s
    assert score.exactitude >= 7.0
    assert score.completude >= 7.0
    assert score.format >= 8.0  # Format APRES respect√©
    assert score.sources >= 8.0  # Sources cit√©es
    assert score.score_global >= 7.0

@pytest.mark.asyncio
async def test_evaluate_bad_answer(openai_client):
    """Test √©valuation d'une mauvaise r√©ponse"""

    judge = LLMJudge(openai_client)

    question = "Combien de cong√©s pay√©s ai-je ?"

    answer = "Je ne sais pas exactement, mais il y a des cong√©s dans le notariat."

    chunks = [
        {'doc_titre': 'CCN Article 45', 'text': 'Les clercs b√©n√©ficient de 30 jours...'}
    ]

    score = await judge.evaluate(question, answer, chunks)

    # V√©rifier scores faibles
    assert score.exactitude < 6.0  # Info impr√©cise
    assert score.completude < 5.0  # Ne r√©pond pas vraiment
    assert score.format < 3.0  # Pas de format APRES
    assert score.sources < 3.0  # Pas de sources
    assert score.score_global < 5.0
```

---

## üìà Impact attendu

### Avant am√©lioration

- ‚ùå Pas d'√©valuation automatique
- ‚ùå Tests manuels chronophages
- ‚ùå R√©gressions d√©tect√©es tardivement

### Apr√®s am√©lioration

- ‚úÖ √âvaluation automatique 24/7
- ‚úÖ Dashboard de qualit√© en temps r√©el
- ‚úÖ Alertes sur r√©ponses faibles
- ‚úÖ Monitoring continu

---

## üìÖ Planning d'impl√©mentation

**Total** : 1 jour

### Matin (4h)

- ‚úÖ Cr√©er `llm_judge.py`
- ‚úÖ Impl√©menter evaluate()
- ‚úÖ Tests unitaires

### Apr√®s-midi (4h)

- ‚úÖ Int√©grer dans notaria_rag_service.py
- ‚úÖ Cr√©er table quality_scores
- ‚úÖ Script generate_quality_dashboard.py
- ‚úÖ D√©ploiement

---

## üéØ Crit√®res de succ√®s

### Crit√®res obligatoires

1. ‚úÖ **√âvaluation automatique** : 100% des r√©ponses √©valu√©es
2. ‚úÖ **Corr√©lation humaine** : Score juge corr√©l√© >80% avec √©valuation humaine
3. ‚úÖ **Dashboard fonctionnel** : Rapport g√©n√©r√© quotidiennement

---

[‚Üê Retour √† l'index](./00_INDEX.md)
