# ğŸ¯ AmÃ©lioration #2 : Reranking Cognitif

[â† Retour Ã  l'index](./00_INDEX.md)

---

## ğŸ“Š Fiche technique

| Attribut | Valeur |
|----------|--------|
| **PrioritÃ©** | ğŸ”¥ CRITIQUE |
| **Impact** | â­â­â­â­â­ (+50% complÃ©tude rÃ©ponses) |
| **Effort** | 1 jour |
| **Statut** | ğŸ“‹ Ã€ faire |
| **DÃ©pendances** | #1 Routage sÃ©mantique (recommandÃ© mais pas obligatoire) |
| **Repo principal** | `application` |

---

## ğŸ”´ ProblÃ¨me identifiÃ©

### SymptÃ´mes
- **20% des Ã©checs** dus Ã  rÃ©ponses incomplÃ¨tes
- Chunks pertinents manquÃ©s
- Contexte tronquÃ© ou imprÃ©cis

### Exemples d'Ã©checs
```
âŒ TEST_USER_001 : "Quelles sont les rÃ¨gles de prÃ©voyance ?"
RÃ©ponse actuelle : Info partielle, manque dÃ©tails sur contributions
Cause : top_k=5 trop faible, chunks importants en position 7-12

âŒ TEST_USER_007 : "Comment gÃ©rer les congÃ©s dans un office ?"
RÃ©ponse actuelle : Manque contexte CCN
Cause : Chunk CCN score 6e position, non inclus dans contexte LLM
```

### Cause racine

**Top-k trop faible + Pas de rÃ©ordonnancement** : La recherche vectorielle ramÃ¨ne 5 chunks, mais :
- Chunks pertinents souvent en position 6-15
- Pas de filtrage par pertinence rÃ©elle Ã  la question
- Envoi direct au LLM sans vÃ©rification

```python
# Code actuel
results = await self.neo4j.vector_search(question, top_k=5)
# âŒ Envoie directement les 5 premiers au LLM
# MÃªme si certains sont peu pertinents et d'autres pertinents manquÃ©s
```

---

## âœ… Solution proposÃ©e

### Principe

**Ã‰largir puis filtrer** : RÃ©cupÃ©rer plus de chunks (20), puis utiliser un LLM pour reranker et sÃ©lectionner les 8 meilleurs.

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Question   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ã‰TAPE 1 : Recherche largeâ”‚
â”‚ top_k = 20 chunks        â”‚  â† ğŸ†• AugmentÃ© de 5 â†’ 20
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ã‰TAPE 2 : Reranking LLM  â”‚  â† ğŸ†• NOUVEAU
â”‚ Score 0-10 par chunk     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ã‰TAPE 3 : SÃ©lection      â”‚  â† ğŸ†• NOUVEAU
â”‚ Top 8 chunks (score >7)  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ã‰TAPE 4 : SynthÃ¨se LLM   â”‚  â† Existant optimisÃ©
â”‚ GÃ©nÃ©ration rÃ©ponse       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ—ï¸ ImplÃ©mentation dÃ©taillÃ©e

### Ã‰TAPE 1 : CrÃ©er le module de reranking

**Repo** : `application`
**Fichier** : `services/reranker_service.py` (ğŸ†• Ã€ crÃ©er)

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Service de reranking des rÃ©sultats de recherche vectorielle
Utilise un LLM pour scorer la pertinence rÃ©elle de chaque chunk
"""

from typing import List, Dict
from openai import AsyncOpenAI
from dataclasses import dataclass


@dataclass
class ScoredChunk:
    """Chunk avec son score de pertinence"""
    text: str
    metadata: Dict
    doc_titre: str
    vector_score: float
    relevance_score: float  # 0-10


RERANKING_PROMPT = """Tu es un expert en analyse de pertinence documentaire pour le notariat franÃ§ais.

Ta tÃ¢che : Ã‰valuer la pertinence d'un extrait de document par rapport Ã  une question.

QUESTION :
{question}

EXTRAIT DE DOCUMENT :
Source : {doc_titre}
Contenu : {chunk_text}

INSTRUCTIONS :
1. Analyse si l'extrait contient des informations directement utiles pour rÃ©pondre Ã  la question
2. Attribue un score de pertinence de 0 Ã  10 :
   - 10 : RÃ©pond directement et complÃ¨tement Ã  la question
   - 7-9 : Contient des Ã©lÃ©ments de rÃ©ponse importants
   - 4-6 : Contexte gÃ©nÃ©ral utile mais pas central
   - 1-3 : Vaguement liÃ© au sujet
   - 0 : Non pertinent

3. PÃ©nalise si :
   - L'extrait est trop gÃ©nÃ©rique
   - L'extrait parle d'un domaine diffÃ©rent
   - L'extrait ne contient que du contexte sans info concrÃ¨te

RÃ‰PONDS UNIQUEMENT AVEC LE SCORE (un nombre entre 0 et 10).
"""


class RerankerService:
    """
    Service de reranking des rÃ©sultats de recherche
    """

    def __init__(self, openai_client: AsyncOpenAI):
        self.client = openai_client

    async def score_chunk(
        self,
        question: str,
        chunk_text: str,
        doc_titre: str
    ) -> float:
        """
        Score la pertinence d'un chunk par rapport Ã  la question

        Args:
            question: Question de l'utilisateur
            chunk_text: Texte du chunk
            doc_titre: Titre du document source

        Returns:
            Score de pertinence (0-10)
        """
        response = await self.client.chat.completions.create(
            model="gpt-4o-mini",  # Rapide et Ã©conomique
            messages=[
                {
                    "role": "user",
                    "content": RERANKING_PROMPT.format(
                        question=question,
                        chunk_text=chunk_text[:500],  # Limiter pour coÃ»t
                        doc_titre=doc_titre
                    )
                }
            ],
            temperature=0,
            max_tokens=5
        )

        try:
            score = float(response.choices[0].message.content.strip())
            # Borner entre 0 et 10
            return max(0.0, min(10.0, score))
        except (ValueError, AttributeError):
            # Si parsing Ã©choue, score neutre
            return 5.0

    async def rerank(
        self,
        question: str,
        chunks: List[Dict],
        top_k: int = 8,
        min_score: float = 7.0
    ) -> List[ScoredChunk]:
        """
        Rerank les chunks par pertinence rÃ©elle

        Args:
            question: Question de l'utilisateur
            chunks: Liste des chunks bruts de la recherche vectorielle
            top_k: Nombre de chunks Ã  retourner
            min_score: Score minimum pour Ãªtre retenu

        Returns:
            Liste des chunks rerankÃ©s et filtrÃ©s
        """
        scored_chunks = []

        # Scorer tous les chunks en parallÃ¨le (asyncio)
        import asyncio
        tasks = []

        for chunk in chunks:
            task = self.score_chunk(
                question=question,
                chunk_text=chunk.get('text', ''),
                doc_titre=chunk.get('doc_titre', 'Document sans titre')
            )
            tasks.append(task)

        # Attendre tous les scores
        scores = await asyncio.gather(*tasks)

        # CrÃ©er les chunks scorÃ©s
        for chunk, relevance_score in zip(chunks, scores):
            scored_chunks.append(ScoredChunk(
                text=chunk.get('text', ''),
                metadata=chunk.get('metadata', {}),
                doc_titre=chunk.get('doc_titre', ''),
                vector_score=chunk.get('score', 0.0),
                relevance_score=relevance_score
            ))

        # Filtrer et trier
        filtered = [c for c in scored_chunks if c.relevance_score >= min_score]
        filtered.sort(key=lambda x: x.relevance_score, reverse=True)

        # Retourner top_k
        return filtered[:top_k]


    async def rerank_hybrid(
        self,
        question: str,
        chunks: List[Dict],
        top_k: int = 8
    ) -> List[ScoredChunk]:
        """
        Reranking hybride : combine score vectoriel et score LLM

        Formule : score_final = 0.3 * vector_score + 0.7 * llm_score
        """
        scored_chunks = []
        import asyncio
        tasks = [
            self.score_chunk(
                question,
                chunk.get('text', ''),
                chunk.get('doc_titre', '')
            )
            for chunk in chunks
        ]

        llm_scores = await asyncio.gather(*tasks)

        for chunk, llm_score in zip(chunks, llm_scores):
            vector_score = chunk.get('score', 0.0)

            # Normaliser vector_score (0-1) â†’ (0-10)
            vector_score_normalized = vector_score * 10

            # Score hybride
            final_score = (
                0.3 * vector_score_normalized +
                0.7 * llm_score
            )

            scored_chunks.append(ScoredChunk(
                text=chunk.get('text', ''),
                metadata=chunk.get('metadata', {}),
                doc_titre=chunk.get('doc_titre', ''),
                vector_score=vector_score,
                relevance_score=final_score
            ))

        # Trier par score final
        scored_chunks.sort(key=lambda x: x.relevance_score, reverse=True)
        return scored_chunks[:top_k]
```

---

### Ã‰TAPE 2 : Modifier le service RAG

**Repo** : `application`
**Fichier** : `services/notaria_rag_service.py` (ğŸ”§ Ã€ modifier)

```python
from services.reranker_service import RerankerService

class NotariaRAGService:
    def __init__(self, ...):
        # ... existant
        self.reranker = RerankerService(openai_client)


    async def query(self, question: str):
        # Ã‰TAPE 1 : Classifier (si amÃ©lioration #1 activÃ©e)
        domain = await self.classifier.classify(question)

        if domain == "HORS_PERIMETRE":
            return self._hors_perimetre_response()

        # Ã‰TAPE 2 : Recherche LARGE
        raw_chunks = await self.neo4j.vector_search_filtered(
            question=question,
            domain_filter=domain,
            top_k=20  # â† ğŸ†• AugmentÃ© de 5 â†’ 20
        )

        # Ã‰TAPE 3 : ğŸ†• RERANKING
        reranked_chunks = await self.reranker.rerank(
            question=question,
            chunks=raw_chunks,
            top_k=8,          # SÃ©lectionne les 8 meilleurs
            min_score=7.0     # Score minimum 7/10
        )

        # Si pas assez de chunks avec score >7, utiliser top 8 quand mÃªme
        if len(reranked_chunks) < 5:
            reranked_chunks = await self.reranker.rerank_hybrid(
                question=question,
                chunks=raw_chunks,
                top_k=8
            )

        # Ã‰TAPE 4 : Construire le contexte pour le LLM final
        context = self._build_context(reranked_chunks)

        # Ã‰TAPE 5 : GÃ©nÃ©rer la rÃ©ponse
        response = await self._generate_answer(question, context)

        return {
            "answer": response,
            "sources": [c.doc_titre for c in reranked_chunks],
            "scores": [c.relevance_score for c in reranked_chunks],
            "domain": domain
        }


    def _build_context(self, scored_chunks: List[ScoredChunk]) -> str:
        """
        Construit le contexte pour le LLM final
        """
        context_parts = []

        for i, chunk in enumerate(scored_chunks, 1):
            context_parts.append(
                f"[Document {i}] {chunk.doc_titre}\n"
                f"Pertinence: {chunk.relevance_score:.1f}/10\n"
                f"{chunk.text}\n"
            )

        return "\n---\n".join(context_parts)
```

---

## ğŸ“Š Gains attendus

### Performance

| MÃ©trique | Avant | AprÃ¨s | AmÃ©lioration |
|----------|-------|-------|--------------|
| **Chunks analysÃ©s** | 5 | 20 â†’ 8 | +60% initiaux, -20% finaux |
| **ComplÃ©tude rÃ©ponses** | 50% | 95% | **+90%** |
| **PrÃ©cision contexte** | 60% | 90% | **+50%** |
| **Taux rÃ©ussite tests** | 34% | 65% | **+91%** |

### QualitÃ©

- âœ… Chunks pertinents en position 6-15 maintenant capturÃ©s
- âœ… Chunks peu pertinents filtrÃ©s (score <7)
- âœ… Contexte LLM final optimisÃ© (8 meilleurs au lieu de 5 moyens)

### CoÃ»t

```
Avant :
- 1 requÃªte Neo4j (top_k=5)
- 1 appel LLM gÃ©nÃ©ration
= ~0.02â‚¬ par question

AprÃ¨s :
- 1 requÃªte Neo4j (top_k=20)
- 20 appels LLM reranking (gpt-4o-mini)
- 1 appel LLM gÃ©nÃ©ration
= ~0.05â‚¬ par question (+150% mais acceptable)
```

**Optimisation coÃ»t possible** :
- Reranking par batch (10 chunks par appel)
- Cache des scores pour questions similaires

---

## ğŸ§ª Tests & Validation

### Test unitaire du reranker

```python
# tests/test_reranker.py
import pytest
from services.reranker_service import RerankerService

@pytest.mark.asyncio
async def test_reranker_score_pertinent():
    """Test que le reranker donne un bon score aux chunks pertinents"""
    reranker = RerankerService(openai_client)

    question = "Quel est le salaire minimum d'un clerc ?"
    chunk_pertinent = "Article 15 de la CCN : Le salaire minimum d'un clerc dÃ©butant est de..."
    chunk_non_pertinent = "Les congÃ©s payÃ©s sont de 25 jours par an..."

    score_pertinent = await reranker.score_chunk(question, chunk_pertinent, "CCN")
    score_non_pertinent = await reranker.score_chunk(question, chunk_non_pertinent, "CCN")

    assert score_pertinent >= 8.0, "Chunk pertinent devrait avoir score >=8"
    assert score_non_pertinent <= 5.0, "Chunk non pertinent devrait avoir score <=5"


@pytest.mark.asyncio
async def test_reranker_full_pipeline():
    """Test du pipeline complet de reranking"""
    reranker = RerankerService(openai_client)

    question = "Comment fonctionne la prÃ©voyance ?"
    chunks = [
        {"text": "La prÃ©voyance couvre les risques dÃ©cÃ¨s, invaliditÃ©...", "doc_titre": "Guide prÃ©voyance", "score": 0.8},
        {"text": "Les congÃ©s payÃ©s sont calculÃ©s...", "doc_titre": "CCN", "score": 0.7},
        {"text": "Le taux de cotisation prÃ©voyance est de 1.5%...", "doc_titre": "Avenant 48", "score": 0.75},
    ]

    reranked = await reranker.rerank(question, chunks, top_k=2, min_score=7.0)

    # VÃ©rifier que seuls les chunks pertinents sont retenus
    assert len(reranked) >= 1
    assert all(c.relevance_score >= 7.0 for c in reranked)
    assert reranked[0].relevance_score >= reranked[-1].relevance_score  # Ordre dÃ©croissant
```

### Test d'intÃ©gration

```python
# tests/test_rag_with_reranking.py
import pytest

@pytest.mark.asyncio
async def test_rag_with_reranking():
    """Test end-to-end avec reranking"""
    rag = NotariaRAGService()

    # Question complexe nÃ©cessitant plusieurs sources
    response = await rag.query(
        "Quelles sont les rÃ¨gles de prÃ©voyance et combien Ã§a coÃ»te ?"
    )

    # VÃ©rifier que la rÃ©ponse est complÃ¨te
    assert "prÃ©voyance" in response['answer'].lower()
    assert any(word in response['answer'].lower() for word in ["taux", "cotisation", "coÃ»t", "%"])

    # VÃ©rifier que plusieurs sources sont utilisÃ©es
    assert len(response['sources']) >= 3

    # VÃ©rifier que les scores sont bons
    assert all(score >= 7.0 for score in response['scores'])
```

---

## ğŸ“Š MÃ©triques de monitoring

### Ã€ tracker en production

```python
# Logs Ã  ajouter dans notaria_rag_service.py
import logging

logger.info("Reranking stats", extra={
    "question_id": question_id,
    "raw_chunks_count": len(raw_chunks),
    "reranked_chunks_count": len(reranked_chunks),
    "min_score": min([c.relevance_score for c in reranked_chunks]),
    "max_score": max([c.relevance_score for c in reranked_chunks]),
    "avg_score": sum([c.relevance_score for c in reranked_chunks]) / len(reranked_chunks),
    "filtered_out": len(raw_chunks) - len(reranked_chunks)
})
```

### Dashboard recommandÃ©

- Distribution des scores de reranking
- Taux de chunks filtrÃ©s (<7/10)
- CorrÃ©lation score vectoriel vs score LLM
- Temps de reranking moyen

---

## ğŸ”„ Rollback si Ã©chec

### DÃ©sactivation rapide

```python
# Configuration
USE_RERANKING = True  # Variable d'environnement

async def query(self, question: str):
    raw_chunks = await self.neo4j.vector_search(..., top_k=20 if USE_RERANKING else 5)

    if USE_RERANKING:
        chunks = await self.reranker.rerank(question, raw_chunks)
    else:
        chunks = raw_chunks[:5]  # Comportement ancien

    # ...
```

### Optimisation progressive

```python
# Mode A/B test
import random

if random.random() < 0.5:
    # 50% avec reranking
    chunks = await self.reranker.rerank(question, raw_chunks)
else:
    # 50% sans reranking
    chunks = raw_chunks[:5]

# Comparer mÃ©triques satisfaction
```

---

## ğŸ“… Planning d'implÃ©mentation

### Demi-journÃ©e 1
- âœ… CrÃ©er `reranker_service.py`
- âœ… Tests unitaires reranker
- âœ… Validation prompt de scoring

### Demi-journÃ©e 2
- âœ… Modifier `notaria_rag_service.py`
- âœ… Augmenter top_k 5â†’20
- âœ… IntÃ©grer reranking dans pipeline
- âœ… Tests d'intÃ©gration
- âœ… Validation sur dataset 15 questions

---

## âœ… Checklist de dÃ©ploiement

- [ ] `reranker_service.py` crÃ©Ã© et testÃ©
- [ ] Tests unitaires reranker : 100% passent
- [ ] Prompt de scoring validÃ© (prÃ©cision >85%)
- [ ] Service RAG modifiÃ© avec reranking
- [ ] top_k augmentÃ© Ã  20 dans recherche vectorielle
- [ ] Tests d'intÃ©gration : 100% passent
- [ ] Logs et mÃ©triques en place
- [ ] CoÃ»t par requÃªte mesurÃ© et validÃ© (<0.10â‚¬)
- [ ] Variable de rollback `USE_RERANKING` en place
- [ ] Validation manuelle sur 15 questions test
- [ ] AmÃ©lioration complÃ©tude mesurÃ©e (>+40%)

---

## ğŸ¯ CritÃ¨res de succÃ¨s

**DÃ©ploiement validÃ© si :**
- âœ… ComplÃ©tude rÃ©ponses passe de 50% â†’ **>90%**
- âœ… Taux de succÃ¨s tests passe de 34% â†’ **>60%** (combinÃ© avec #1)
- âœ… Score moyen reranking **>7.5/10**
- âœ… CoÃ»t par requÃªte **<0.10â‚¬**
- âœ… Latence ajoutÃ©e **<2 secondes**

---

[â† Retour Ã  l'index](./00_INDEX.md) | [AmÃ©lioration suivante : Gestion des limites â†’](./03_gestion_limites.md)
