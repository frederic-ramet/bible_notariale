# ğŸ“„ AmÃ©lioration #9 : Chunking SÃ©mantique

[â† Retour Ã  l'index](./00_INDEX.md)

---

## ğŸ“Š Fiche technique

| Attribut | Valeur |
|----------|--------|
| **PrioritÃ©** | ğŸŸ¡ LONG TERME |
| **Impact** | â­â­â­ (QualitÃ© des chunks) |
| **Effort** | 1.5 jours |
| **Statut** | ğŸ“‹ Ã€ faire |
| **DÃ©pendances** | Aucune (indÃ©pendant) |
| **Repo** | `application` |

---

## ğŸ¯ ProblÃ¨me identifiÃ©

### Observations

**ProblÃ¨me** : Le chunking actuel coupe les documents de maniÃ¨re arbitraire (taille fixe)

**SymptÃ´mes** :
- Chunks coupÃ©s au milieu d'une phrase ou d'un paragraphe
- Perte de contexte sÃ©mantique
- Informations fragmentÃ©es entre plusieurs chunks

**Impact** :
- âŒ Chunks non cohÃ©rents sÃ©mantiquement
- âŒ Contexte perdu (chunk ne fait pas sens seul)
- âŒ LLM doit reconstituer le sens depuis plusieurs chunks

**Exemple concret** :

```
Document original :

"Article 45 - CongÃ©s payÃ©s

Les clercs de notaire bÃ©nÃ©ficient de 30 jours ouvrables de congÃ©s payÃ©s par an,
acquis Ã  raison de 2.5 jours par mois de travail effectif.

La pÃ©riode de rÃ©fÃ©rence court du 1er juin de l'annÃ©e N au 31 mai de l'annÃ©e N+1.

Les congÃ©s doivent Ãªtre pris..."

âŒ Chunking par taille fixe (200 caractÃ¨res) :

Chunk 1 : "Article 45 - CongÃ©s payÃ©s\n\nLes clercs de notaire bÃ©nÃ©ficient de 30 jours
ouvrables de congÃ©s payÃ©s par an, acquis Ã  raison de 2.5 jours par mois de travail effectif.\n\nLa pÃ©riode de rÃ©"

Chunk 2 : "fÃ©rence court du 1er juin de l'annÃ©e N au 31 mai de l'annÃ©e N+1.\n\nLes congÃ©s doivent Ãªtre pris..."

â†’ Chunk 1 coupÃ© au milieu de "rÃ©fÃ©rence"
â†’ Chunk 2 commence par "fÃ©rence" (incomprÃ©hensible seul)

âœ… Chunking sÃ©mantique :

Chunk 1 : "Article 45 - CongÃ©s payÃ©s\n\nLes clercs de notaire bÃ©nÃ©ficient de 30 jours
ouvrables de congÃ©s payÃ©s par an, acquis Ã  raison de 2.5 jours par mois de travail effectif."

Chunk 2 : "La pÃ©riode de rÃ©fÃ©rence court du 1er juin de l'annÃ©e N au 31 mai de l'annÃ©e N+1."

Chunk 3 : "Les congÃ©s doivent Ãªtre pris..."

â†’ Chaque chunk est sÃ©mantiquement cohÃ©rent
â†’ Peut Ãªtre compris indÃ©pendamment
```

---

## ğŸ’¡ Solution proposÃ©e

### Vue d'ensemble

**Chunking sÃ©mantique hiÃ©rarchique** :

1. **Niveau 1** : DÃ©coupage par sections (titres, articles)
2. **Niveau 2** : DÃ©coupage par paragraphes
3. **Niveau 3** : Fusion si trop petit, split si trop grand

### StratÃ©gie

```python
# RÃ¨gles de chunking sÃ©mantique

1. Si document a une structure (articles, sections) :
   â†’ DÃ©couper par article/section
   â†’ Garder le titre avec le contenu

2. Sinon, dÃ©couper par paragraphes :
   â†’ Chaque paragraphe = 1 chunk potentiel
   â†’ Fusionner paragraphes courts (< 100 caractÃ¨res)

3. Respecter taille cible :
   â†’ Minimum : 200 caractÃ¨res
   â†’ Optimal : 400-600 caractÃ¨res
   â†’ Maximum : 1000 caractÃ¨res

4. PrÃ©server contexte :
   â†’ Ajouter overlap de 50 caractÃ¨res entre chunks
   â†’ Inclure titre de section dans chaque chunk
```

---

## ğŸ”§ ImplÃ©mentation dÃ©taillÃ©e

### Nouveau service : `services/semantic_chunker.py`

```python
"""
Chunking sÃ©mantique pour documents
"""

import re
from typing import List, Dict
from dataclasses import dataclass


@dataclass
class Chunk:
    """ReprÃ©sente un chunk sÃ©mantique"""
    text: str
    start_char: int
    end_char: int
    section_title: str = None
    metadata: dict = None


class SemanticChunker:
    """
    DÃ©coupe les documents de maniÃ¨re sÃ©mantique
    """

    def __init__(
        self,
        min_chunk_size: int = 200,
        optimal_chunk_size: int = 500,
        max_chunk_size: int = 1000,
        overlap: int = 50
    ):
        self.min_chunk_size = min_chunk_size
        self.optimal_chunk_size = optimal_chunk_size
        self.max_chunk_size = max_chunk_size
        self.overlap = overlap

    def chunk_document(self, text: str, doc_metadata: dict = None) -> List[Chunk]:
        """
        DÃ©coupe un document en chunks sÃ©mantiques

        Args:
            text: Texte du document
            doc_metadata: MÃ©tadonnÃ©es du document

        Returns:
            Liste de chunks
        """

        # 1. DÃ©tecter structure (articles, sections)
        sections = self._detect_sections(text)

        if sections:
            # Document structurÃ© â†’ chunking par sections
            chunks = self._chunk_by_sections(sections)
        else:
            # Document non structurÃ© â†’ chunking par paragraphes
            chunks = self._chunk_by_paragraphs(text)

        # 2. Post-traitement : fusionner/splitter si nÃ©cessaire
        chunks = self._optimize_chunk_sizes(chunks)

        # 3. Ajouter overlap entre chunks
        chunks = self._add_overlap(chunks, text)

        # 4. Ajouter mÃ©tadonnÃ©es
        for chunk in chunks:
            chunk.metadata = doc_metadata or {}

        return chunks

    def _detect_sections(self, text: str) -> List[Dict]:
        """
        DÃ©tecte les sections structurÃ©es (articles, titres)

        Returns:
            Liste de {title, content, start_pos}
        """

        sections = []

        # Pattern 1 : Articles (ex: "Article 45 - CongÃ©s payÃ©s")
        article_pattern = r'^(Article\s+\d+[A-Z]?\s*-?\s*[^\n]+)\n(.+?)(?=^Article\s+\d+|$)'

        for match in re.finditer(article_pattern, text, re.MULTILINE | re.DOTALL):
            title = match.group(1).strip()
            content = match.group(2).strip()
            start_pos = match.start()

            sections.append({
                'title': title,
                'content': content,
                'start_pos': start_pos
            })

        if sections:
            return sections

        # Pattern 2 : Sections numÃ©rotÃ©es (ex: "1. Introduction", "2. Conditions")
        section_pattern = r'^(\d+\.\s+[^\n]+)\n(.+?)(?=^\d+\.\s+|$)'

        for match in re.finditer(section_pattern, text, re.MULTILINE | re.DOTALL):
            title = match.group(1).strip()
            content = match.group(2).strip()
            start_pos = match.start()

            sections.append({
                'title': title,
                'content': content,
                'start_pos': start_pos
            })

        if sections:
            return sections

        # Pattern 3 : Titres en majuscules
        heading_pattern = r'^([A-ZÃ€Ã‚Ã„Ã‰ÃˆÃŠÃ‹ÃÃÃ”Ã–Ã™Ã›ÃœÅ¸Ã‡\s]{5,})\n(.+?)(?=^[A-ZÃ€Ã‚Ã„Ã‰ÃˆÃŠÃ‹ÃÃÃ”Ã–Ã™Ã›ÃœÅ¸Ã‡\s]{5,}|$)'

        for match in re.finditer(heading_pattern, text, re.MULTILINE | re.DOTALL):
            title = match.group(1).strip()
            content = match.group(2).strip()
            start_pos = match.start()

            sections.append({
                'title': title,
                'content': content,
                'start_pos': start_pos
            })

        return sections

    def _chunk_by_sections(self, sections: List[Dict]) -> List[Chunk]:
        """
        DÃ©coupe par sections dÃ©tectÃ©es
        """

        chunks = []

        for section in sections:
            title = section['title']
            content = section['content']
            start_pos = section['start_pos']

            # Si section trop grande, dÃ©couper par paragraphes
            if len(content) > self.max_chunk_size:
                para_chunks = self._chunk_by_paragraphs(content)

                # Ajouter titre Ã  chaque chunk
                for chunk in para_chunks:
                    chunk.text = f"{title}\n\n{chunk.text}"
                    chunk.section_title = title
                    chunk.start_char += start_pos

                chunks.extend(para_chunks)
            else:
                # Section assez petite, garder entiÃ¨re
                chunk = Chunk(
                    text=f"{title}\n\n{content}",
                    start_char=start_pos,
                    end_char=start_pos + len(title) + len(content) + 2,
                    section_title=title
                )
                chunks.append(chunk)

        return chunks

    def _chunk_by_paragraphs(self, text: str) -> List[Chunk]:
        """
        DÃ©coupe par paragraphes
        """

        # Splitter par double saut de ligne
        paragraphs = re.split(r'\n\s*\n', text)

        chunks = []
        current_chunk = ""
        current_start = 0

        for i, para in enumerate(paragraphs):
            para = para.strip()
            if not para:
                continue

            # Si ajout de ce paragraphe dÃ©passe la taille optimale â†’ crÃ©er chunk
            if len(current_chunk) + len(para) > self.optimal_chunk_size and current_chunk:
                chunk = Chunk(
                    text=current_chunk.strip(),
                    start_char=current_start,
                    end_char=current_start + len(current_chunk)
                )
                chunks.append(chunk)

                current_chunk = para
                current_start = text.find(para, current_start)
            else:
                # Ajouter paragraphe au chunk courant
                if current_chunk:
                    current_chunk += "\n\n" + para
                else:
                    current_chunk = para
                    current_start = text.find(para)

        # Dernier chunk
        if current_chunk:
            chunk = Chunk(
                text=current_chunk.strip(),
                start_char=current_start,
                end_char=current_start + len(current_chunk)
            )
            chunks.append(chunk)

        return chunks

    def _optimize_chunk_sizes(self, chunks: List[Chunk]) -> List[Chunk]:
        """
        Optimise la taille des chunks :
        - Fusionne chunks trop petits
        - Split chunks trop grands
        """

        optimized = []
        i = 0

        while i < len(chunks):
            chunk = chunks[i]

            # Chunk trop petit â†’ fusionner avec le suivant
            if len(chunk.text) < self.min_chunk_size and i + 1 < len(chunks):
                next_chunk = chunks[i + 1]

                merged = Chunk(
                    text=chunk.text + "\n\n" + next_chunk.text,
                    start_char=chunk.start_char,
                    end_char=next_chunk.end_char,
                    section_title=chunk.section_title
                )

                optimized.append(merged)
                i += 2  # Sauter le chunk suivant (fusionnÃ©)

            # Chunk trop grand â†’ splitter
            elif len(chunk.text) > self.max_chunk_size:
                # Split au niveau des phrases
                sentences = re.split(r'(?<=[.!?])\s+', chunk.text)

                sub_chunk = ""
                sub_start = chunk.start_char

                for sentence in sentences:
                    if len(sub_chunk) + len(sentence) > self.optimal_chunk_size and sub_chunk:
                        optimized.append(Chunk(
                            text=sub_chunk.strip(),
                            start_char=sub_start,
                            end_char=sub_start + len(sub_chunk),
                            section_title=chunk.section_title
                        ))

                        sub_chunk = sentence
                        sub_start = sub_start + len(sub_chunk)
                    else:
                        sub_chunk += " " + sentence

                # Dernier sous-chunk
                if sub_chunk:
                    optimized.append(Chunk(
                        text=sub_chunk.strip(),
                        start_char=sub_start,
                        end_char=chunk.end_char,
                        section_title=chunk.section_title
                    ))

                i += 1

            else:
                # Taille OK
                optimized.append(chunk)
                i += 1

        return optimized

    def _add_overlap(self, chunks: List[Chunk], original_text: str) -> List[Chunk]:
        """
        Ajoute un overlap entre chunks pour prÃ©server le contexte
        """

        if self.overlap == 0:
            return chunks

        overlapped = []

        for i, chunk in enumerate(chunks):
            # Ajouter overlap avec chunk prÃ©cÃ©dent
            if i > 0:
                prev_chunk = chunks[i - 1]
                overlap_start = max(0, prev_chunk.end_char - self.overlap)
                overlap_text = original_text[overlap_start:prev_chunk.end_char]

                chunk.text = f"... {overlap_text}\n\n{chunk.text}"

            overlapped.append(chunk)

        return overlapped


# Fonction utilitaire pour rÃ©-indexer avec chunking sÃ©mantique
async def reindex_with_semantic_chunking(
    documents: List[Dict],
    chunker: SemanticChunker
):
    """
    RÃ©-indexe tous les documents avec chunking sÃ©mantique
    """

    all_chunks = []

    for doc in documents:
        doc_id = doc['document_id']
        text = doc['text']  # Texte extrait du PDF
        metadata = doc.get('classification', {})

        # Chunking sÃ©mantique
        chunks = chunker.chunk_document(text, metadata)

        # PrÃ©parer pour Neo4j
        for i, chunk in enumerate(chunks):
            all_chunks.append({
                'doc_id': doc_id,
                'chunk_id': f"{doc_id}_chunk_{i}",
                'text': chunk.text,
                'section_title': chunk.section_title,
                'chunk_index': i,
                'metadata': chunk.metadata
            })

    return all_chunks
```

---

## âœ… Tests et validation

### Tests unitaires

```python
"""
Tests pour chunking sÃ©mantique
"""

import pytest
from services.semantic_chunker import SemanticChunker

def test_chunk_by_articles():
    """Test chunking d'un document avec articles"""

    text = """Article 45 - CongÃ©s payÃ©s

Les clercs de notaire bÃ©nÃ©ficient de 30 jours ouvrables de congÃ©s payÃ©s par an.

La pÃ©riode de rÃ©fÃ©rence court du 1er juin au 31 mai.

Article 46 - CongÃ©s exceptionnels

Des congÃ©s exceptionnels sont accordÃ©s dans les cas suivants :
- Mariage : 4 jours
- Naissance : 3 jours
"""

    chunker = SemanticChunker(min_chunk_size=50, optimal_chunk_size=200, max_chunk_size=500)
    chunks = chunker.chunk_document(text)

    # Devrait crÃ©er 2 chunks (1 par article)
    assert len(chunks) >= 2

    # Chaque chunk doit contenir le titre
    assert "Article 45" in chunks[0].text
    assert "Article 46" in chunks[1].text

    # Pas de coupure au milieu d'un mot
    for chunk in chunks:
        assert not chunk.text.startswith(' ')
        assert not chunk.text.endswith(' ')

def test_chunk_optimization():
    """Test fusion de chunks trop petits"""

    text = """Petit paragraphe 1.

Petit paragraphe 2.

Petit paragraphe 3.
"""

    chunker = SemanticChunker(min_chunk_size=100, optimal_chunk_size=200)
    chunks = chunker.chunk_document(text)

    # Les petits paragraphes doivent Ãªtre fusionnÃ©s
    assert len(chunks) == 1
    assert "Petit paragraphe 1" in chunks[0].text
    assert "Petit paragraphe 2" in chunks[0].text
```

---

## ğŸ“ˆ Impact attendu

### Avant amÃ©lioration

- âŒ Chunks coupÃ©s arbitrairement
- âŒ Perte de contexte sÃ©mantique
- âŒ Chunks incomprÃ©hensibles seuls

### AprÃ¨s amÃ©lioration

- âœ… Chunks sÃ©mantiquement cohÃ©rents
- âœ… Contexte prÃ©servÃ© (titre + contenu)
- âœ… Meilleure qualitÃ© de rÃ©cupÃ©ration

---

## ğŸ“… Planning d'implÃ©mentation

**Total** : 1.5 jours

### Jour 1 (8h)

- âœ… CrÃ©er `semantic_chunker.py`
- âœ… ImplÃ©menter dÃ©tection sections
- âœ… ImplÃ©menter chunking par paragraphes
- âœ… Tests unitaires

### Jour 2 (4h)

- âœ… RÃ©-indexation complÃ¨te avec nouveau chunking
- âœ… Tests manuels comparatifs
- âœ… Validation qualitÃ© chunks

---

[â† Retour Ã  l'index](./00_INDEX.md) | [AmÃ©lioration suivante : Filtrage temporel â†’](./10_filtrage_temporel.md)
