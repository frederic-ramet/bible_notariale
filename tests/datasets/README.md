# Dataset de Test pour le Chatbot Bible Notariale

## üìã Vue d'ensemble

Ce r√©pertoire contient le dataset de test pour valider les performances du futur chatbot RAG (Retrieval-Augmented Generation) de la Bible Notariale. L'objectif est de s'assurer que le chatbot cite les bonnes sources documentaires et fournit des r√©ponses pertinentes aux questions des notaires.

## üéØ Objectifs du dataset

### Validation multi-niveaux
1. **Pr√©cision de r√©cup√©ration** : Le chatbot identifie-t-il les bons documents sources ?
2. **Pr√©cision de citation** : Le chatbot cite-t-il correctement les r√©f√©rences (circulaires, avenants, articles) ?
3. **Pertinence de la r√©ponse** : Les √©l√©ments cl√©s attendus sont-ils pr√©sents dans la r√©ponse ?
4. **Gestion des cas limites** : Comment le chatbot se comporte-t-il face √† des questions hors p√©rim√®tre ou tr√®s larges ?

### Couverture th√©matique
- **70% d√©ontologie** (35 questions) : c≈ìur de m√©tier des questions √† la Chambre des Notaires
- **20% juridique sp√©cifique** (10 questions) : CCN, avenants, statut, organisation professionnelle
- **10% edge cases** (5 questions) : questions larges, hors p√©rim√®tre, comportement du chatbot

## üìä Structure du dataset

Le fichier `chatbot_test_dataset.json` contient un tableau de questions structur√©es comme suit :

```json
{
  "id": "Q001",
  "categorie": "deontologie|juridique|edge_case",
  "difficulte": "facile|moyen|pointu",
  "question": "La question pos√©e par le notaire",
  "documents_sources_attendus": [
    "document_id_1",
    "document_id_2"
  ],
  "elements_cles_reponse": [
    "Point cl√© 1 qui devrait appara√Ætre dans la r√©ponse",
    "Point cl√© 2 qui devrait appara√Ætre dans la r√©ponse"
  ],
  "reponse_attendue_resumee": "R√©sum√© en 2-3 phrases de ce que devrait r√©pondre le chatbot",
  "articles_references": [
    "Article X du Code de d√©ontologie",
    "Article Y du RPN"
  ],
  "necessite_multi_documents": false,
  "notes_validation": "Notes de l'expert m√©tier apr√®s validation"
}
```

## üèóÔ∏è M√©thodologie de cr√©ation

### 1. Sources d'inspiration
Les questions ont √©t√© cr√©√©es en analysant :
- Les 245 documents index√©s dans `_metadata/index_complet.json`
- Les ~1000 questions typiques d√©j√† pr√©sentes dans les m√©tadonn√©es individuelles
- Focus particulier sur :
  - Circulaires CSN (20 documents)
  - Guides pratiques (28 documents)
  - Fil-Info (153 documents)
  - Documents sp√©cifiques √† la d√©ontologie et au RPN

### 2. R√©partition par difficult√©

#### Questions faciles (35%)
- D√©finitions de base
- R√®gles d√©ontologiques simples
- Questions √† r√©ponse directe dans un seul document
- Ex: "Qu'est-ce que la LCB-FT ?"

#### Questions moyennes (40%)
- Situations pratiques courantes
- Interpr√©tation de r√®gles
- N√©cessitent de croiser quelques informations
- Ex: "Quelles sont les obligations du notaire en mati√®re de m√©diation de la consommation ?"

#### Questions pointues (25%)
- Cas complexes ou rares
- Interpr√©tation fine de textes
- Croisement de plusieurs documents
- R√©f√©rences juridiques pr√©cises
- Ex: "Dans quel cas un notaire peut-il d√©roger au secret professionnel selon l'article X du code de d√©ontologie ?"

### 3. Vari√©t√© des types de questions

- **Questions factuelles** : recherche d'information pr√©cise
- **Questions proc√©durales** : "Comment faire X ?"
- **Questions d'interpr√©tation** : "Dans quel cas puis-je..."
- **Questions de r√©f√©rences** : "Quel article traite de..."
- **Questions temporelles** : "Qu'est-ce qui a chang√© en 2024 ?"
- **Questions multi-documents** : n√©cessitent de croiser plusieurs sources
- **Questions hors p√©rim√®tre** : pour tester les limites du chatbot

## üîÑ Workflow d'utilisation

### Phase 1 : Cr√©ation initiale ‚úÖ
1. Analyse des documents de la Bible Notariale
2. G√©n√©ration de 50 questions avec m√©tadonn√©es compl√®tes
3. Export JSON structur√©

### Phase 2 : Validation m√©tier üìã
1. Transmission du dataset √† un expert m√©tier (notaire senior, d√©ontologue)
2. Validation de :
   - Pertinence des questions
   - Exactitude des documents sources attendus
   - Compl√©tude des √©l√©ments cl√©s de r√©ponse
   - Pertinence des r√©ponses attendues
3. Ajout de notes dans le champ `notes_validation`
4. Correction/enrichissement si n√©cessaire

### Phase 3 : Test avec le chatbot ü§ñ
1. Impl√©mentation du chatbot RAG
2. Pour chaque question du dataset :
   - Soumission au chatbot
   - Collecte de la r√©ponse et des sources cit√©es
   - Comparaison avec les valeurs attendues
3. Calcul des m√©triques de performance

## üìà M√©triques d'√©valuation

### M√©triques de r√©cup√©ration
- **Recall@K** : % de documents pertinents retrouv√©s dans les K premiers r√©sultats
- **Precision@K** : % de documents retrouv√©s qui sont pertinents
- **MRR (Mean Reciprocal Rank)** : position moyenne du premier document pertinent

### M√©triques de r√©ponse
- **Pr√©sence des √©l√©ments cl√©s** : % d'√©l√©ments cl√©s mentionn√©s dans la r√©ponse
- **Exactitude des citations** : % de citations correctement attribu√©es
- **Compl√©tude** : la r√©ponse couvre-t-elle tous les aspects attendus ?

### M√©triques de comportement
- **Taux de refus appropri√©** : % de questions hors p√©rim√®tre correctement identifi√©es
- **Confiance calibr√©e** : le chatbot exprime-t-il une confiance proportionnelle √† la qualit√© de sa r√©ponse ?

## üõ†Ô∏è Utilisation du dataset

### Format de test automatis√© (exemple Python)
```python
import json

# Charger le dataset
with open('chatbot_test_dataset.json', 'r', encoding='utf-8') as f:
    dataset = json.load(f)

# Tester chaque question
results = []
for qa in dataset['qa_pairs']:
    # Soumettre au chatbot
    response = chatbot.query(qa['question'])

    # Comparer avec les attentes
    score = evaluate_response(
        response=response,
        expected_docs=qa['documents_sources_attendus'],
        expected_keys=qa['elements_cles_reponse']
    )

    results.append({
        'question_id': qa['id'],
        'score': score,
        'sources_found': response.sources,
        'missing_keys': identify_missing_keys(response, qa)
    })

# G√©n√©rer rapport
generate_evaluation_report(results)
```

## üìù Notes importantes

### √âvolution du dataset
Ce dataset est **vivant** et doit √™tre enrichi au fil du temps :
- Ajout de nouvelles questions bas√©es sur les cas r√©els
- Ajustement des r√©ponses attendues selon les retours m√©tier
- Mise √† jour lors de l'ajout de nouveaux documents √† la Bible Notariale
- Versionnage du dataset pour tracer les √©volutions

### Limitations connues
- Les questions sont en fran√ßais uniquement
- Focus sur la d√©ontologie et le droit notarial fran√ßais
- Certaines questions peuvent avoir plusieurs r√©ponses valides
- La validation m√©tier est essentielle avant utilisation

### Maintenance
- **Propri√©taire** : √âquipe produit Bible Notariale
- **Fr√©quence de mise √† jour** : Trimestrielle ou lors d'ajouts documentaires majeurs
- **Validation** : Par expert m√©tier √† chaque mise √† jour

## üìö R√©f√©rences

- Index complet des documents : `/_metadata/index_complet.json`
- Vocabulaire notarial : `/_metadata/vocabulaire_notarial.json`
- Documentation technique : `/_INSTRUCTIONS/PLAN_ACTION_INDEX.md`

---

**Derni√®re mise √† jour** : 2025-11-18
**Version du dataset** : 1.0
**Nombre de questions** : 50
